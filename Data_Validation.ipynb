{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9ea188-ffc0-41f1-96bd-5399ecb1e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Postgres folder: raw-data/source=postgres_rds/type=customer_churn/timestamp=2025-02-26/\n",
      "Latest Google Drive folder: raw-data/source=google_drive/type=customer_churn/timestamp=2025-02-26/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "log_file = f'/home/ubuntu/DMML_project/Logs/Data_Validation/validation_{timestamp}.log'\n",
    "\n",
    "def log_message(message):\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} - {message}\\n')\n",
    "\n",
    "ACCESS_KEY = 'AKIAWPPO6VXLYSOLWFE7'\n",
    "SECRET_KEY = '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'\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
    "\n",
    "def get_latest_timestamp_folder(bucket_name, prefix):\n",
    "    log_message(f'Getting latest timestamp folder for {prefix}')\n",
    "    response = s3.list_objects(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "    timestamp_folders = [obj['Prefix'] for obj in response['CommonPrefixes'] if 'timestamp=' in obj['Prefix']]\n",
    "    \n",
    "    latest_timestamp_folder = max(timestamp_folders, key=lambda x: datetime.strptime(x.split('/')[-2], 'timestamp=%Y-%m-%d'))\n",
    "    log_message(f'Latest timestamp folder: {latest_timestamp_folder}')\n",
    "    \n",
    "    return latest_timestamp_folder\n",
    "\n",
    "def copy_latest_csv_to_s3(bucket_name, prefix, target_key):\n",
    "    latest_folder = get_latest_timestamp_folder(bucket_name, prefix)\n",
    "    latest_csv_file = latest_folder + 'customer_churn.csv'\n",
    "    log_message(f'Copying {latest_csv_file} to {target_key}')\n",
    "    s3.copy_object(Bucket=bucket_name, CopySource={'Bucket': bucket_name, 'Key': latest_csv_file}, Key=target_key)\n",
    "\n",
    "def validate_data(df, file_name):\n",
    "    log_message(f'Validating data for {file_name}')\n",
    "    # Check for missing values\n",
    "    null_counts = df.isnull().sum().reset_index()\n",
    "    null_counts.columns = ['Column', 'Null Count']\n",
    "    null_counts['File Name'] = file_name\n",
    "    log_message(f'Null counts for {file_name}: \\n{null_counts}')\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    log_message(f'Duplicate rows for {file_name}: {duplicate_count}')\n",
    "    duplicate_report = pd.DataFrame({'File Name': [file_name], 'Duplicate Rows': [duplicate_count]})\n",
    "    \n",
    "    return null_counts, duplicate_report\n",
    "\n",
    "log_message('Validation started')\n",
    "\n",
    "bucket_name = 'dmml-storage-bits'\n",
    "\n",
    "prefix_postgres = 'raw-data/source=postgres_rds/type=customer_churn/'\n",
    "prefix_google = 'raw-data/source=google_drive/type=customer_churn/'\n",
    "\n",
    "latest_folder_postgres = get_latest_timestamp_folder(bucket_name, prefix_postgres)\n",
    "latest_folder_google = get_latest_timestamp_folder(bucket_name, prefix_google)\n",
    "\n",
    "print(\"Latest Postgres folder:\", latest_folder_postgres)\n",
    "print(\"Latest Google Drive folder:\", latest_folder_google)\n",
    "\n",
    "# Get CSV files from S3\n",
    "postgres_csv_file = latest_folder_postgres + 'customer_churn.csv'\n",
    "google_csv_file = latest_folder_google + 'customer_churn.csv'\n",
    "\n",
    "postgres_df = pd.read_csv(s3.get_object(Bucket=bucket_name, Key=postgres_csv_file)['Body'])\n",
    "google_df = pd.read_csv(s3.get_object(Bucket=bucket_name, Key=google_csv_file)['Body'])\n",
    "\n",
    "# Validate data\n",
    "postgres_null_counts, postgres_duplicate_report = validate_data(postgres_df, 'Postgres')\n",
    "google_null_counts, google_duplicate_report = validate_data(google_df, 'Google Drive')\n",
    "\n",
    "# Save validation reports to CSV\n",
    "null_counts_report = pd.concat([postgres_null_counts, google_null_counts])\n",
    "duplicate_report = pd.concat([postgres_duplicate_report, google_duplicate_report])\n",
    "\n",
    "null_counts_report.to_csv('null_counts_report.csv', index=False)\n",
    "duplicate_report.to_csv('duplicate_report.csv', index=False)\n",
    "\n",
    "# Copy latest CSV files to S3\n",
    "copy_latest_csv_to_s3(bucket_name, prefix_postgres, 'pre_processed/source/customer_churn_postgres.csv')\n",
    "copy_latest_csv_to_s3(bucket_name, prefix_google, 'pre_processed/source/customer_churn_gdrive.csv')\n",
    "\n",
    "log_message('Validation complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc8c05-1a9c-4571-b569-57cfd08446dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

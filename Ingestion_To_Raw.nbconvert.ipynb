{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e2585d-6855-4127-b5fd-6e999f5f9378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T12:00:11.360797Z",
     "iopub.status.busy": "2025-03-04T12:00:11.360453Z",
     "iopub.status.idle": "2025-03-04T12:00:18.539531Z",
     "shell.execute_reply": "2025-03-04T12:00:18.538711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1XHN3wvuqvq-zrH_jr3YWC-RR4flXKIW7\n",
      "To: /home/ubuntu/DMML_project/customer_churn.csv\n",
      "100%|██████████| 1.40M/1.40M [00:00<00:00, 20.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import psycopg2\n",
    "import boto3\n",
    "import logging\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamp-based filename for the log file\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"/home/ubuntu/DMML_project/Logs/Ingestion_To_Raw/ingestion_{timestamp}.log\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "# Google Drive file ID\n",
    "file_id = '1XHN3wvuqvq-zrH_jr3YWC-RR4flXKIW7'\n",
    "\n",
    "# Download CSV file from Google Drive\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "try:\n",
    "    gdown.download(url, 'customer_churn.csv', quiet=False)\n",
    "    logging.info(f\"File from Google Drive saved to customer_churn.csv\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error downloading file from Google Drive: {e}\")\n",
    "\n",
    "# Read CSV file\n",
    "try:\n",
    "    df_gdrive = pd.read_csv('customer_churn.csv')\n",
    "    logging.info(f\"Data from Google Drive loaded into DataFrame\")\n",
    "    logging.info(\"First 5 rows of Google Drive data:\")\n",
    "    logging.info(df_gdrive.head(5).to_string())\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading data from Google Drive: {e}\")\n",
    "\n",
    "# Postgres RDS connection details\n",
    "host = 'database-dmml.cluster-czyuk8c4op6k.eu-north-1.rds.amazonaws.com'\n",
    "port = 5432\n",
    "database = 'postgres'\n",
    "username = 'postgres'\n",
    "password = 'dmml-project-postgres'\n",
    "schema = 'public'\n",
    "\n",
    "# Connect to Postgres RDS\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        user=username,\n",
    "        password=password\n",
    "    )\n",
    "    logging.info(f\"Connected to Postgres RDS\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error connecting to Postgres RDS: {e}\")\n",
    "\n",
    "# Fetch data from Postgres RDS\n",
    "try:\n",
    "    cur = conn.cursor()\n",
    "    query = f\"SELECT * FROM {schema}.customer_churn_db\"\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    columns = [desc[0] for desc in cur.description]\n",
    "    df_postgres = pd.DataFrame(rows, columns=columns)\n",
    "    logging.info(f\"Data from Postgres RDS loaded into DataFrame\")\n",
    "    logging.info(\"First 5 rows of Postgres RDS data:\")\n",
    "    logging.info(df_postgres.head(5).to_string())\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error fetching data from Postgres RDS: {e}\")\n",
    "\n",
    "# Close Postgres RDS connection\n",
    "if conn is not None:\n",
    "    conn.close()\n",
    "    logging.info(f\"Postgres RDS connection closed\")\n",
    "\n",
    "# S3 credentials\n",
    "ACCESS_KEY = 'AKIAWPPO6VXLYSOLWFE7'\n",
    "SECRET_KEY = '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'\n",
    "BUCKET_NAME = 'dmml-storage-bits'\n",
    "REGION = 'eu-north-1'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY,\n",
    "                      aws_secret_access_key=SECRET_KEY,\n",
    "                      region_name=REGION)\n",
    "\n",
    "# Upload DataFrames to S3\n",
    "try:\n",
    "    # Google Drive data\n",
    "    s3_file = f\"raw-data/source=google_drive/type=customer_churn/timestamp={pd.Timestamp.now().strftime('%Y-%m-%d')}/customer_churn.csv\"\n",
    "    csv_buffer = StringIO()\n",
    "    df_gdrive.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=s3_file)\n",
    "    logging.info(f\"Google Drive data uploaded to S3: {s3_file}\")\n",
    "\n",
    "    # Postgres RDS data\n",
    "    s3_file = f\"raw-data/source=postgres_rds/type=customer_churn/timestamp={pd.Timestamp.now().strftime('%Y-%m-%d')}/customer_churn.csv\"\n",
    "    csv_buffer = StringIO()\n",
    "    df_postgres.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=s3_file)\n",
    "    logging.info(f\"Postgres RDS data uploaded to S3: {s3_file}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error uploading data to S3: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ada8ec-cf5b-41f8-b69d-f2441fa17919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
